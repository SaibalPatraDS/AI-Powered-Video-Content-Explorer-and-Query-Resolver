{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b11e1e4",
   "metadata": {},
   "source": [
    "# RAG Application using Google GenAi and Pinecone\n",
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443303b0",
   "metadata": {},
   "source": [
    "Basic Idea : \n",
    "\n",
    "Taking `Youtube` video URL from user and then using `whisper` from `OPENAI` to transcribe the video transcript and then using that transcript to answer questions regarding the video, building a RAG Application. \n",
    "\n",
    "Tech Stack : ``` 1. Langchain\n",
    "             2. Openai\n",
    "             3. HuggingFace\n",
    "             4. Pinecone\n",
    "             5. Google GenAI\n",
    "             ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ac5f3",
   "metadata": {},
   "source": [
    "### 1. Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25849c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GENAI_API_KEY = os.getenv(\"GENAI_API_KEY\")\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model='gemini-pro',\n",
    "                               google_api_key=GENAI_API_KEY)\n",
    "\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87d7006b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='International Criminal Court', response_metadata={'prompt_feedback': {'safety_ratings': [{'category': 9, 'probability': 1, 'blocked': False}, {'category': 8, 'probability': 1, 'blocked': False}, {'category': 7, 'probability': 1, 'blocked': False}, {'category': 10, 'probability': 1, 'blocked': False}], 'block_reason': 0}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the model\n",
    "\n",
    "model.invoke(\"What is the full form of ICC?\")\n",
    "# model.invoke(\"abcd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de50edbb",
   "metadata": {},
   "source": [
    "The `AIMessage` contains the answer and use of `parser` to be done to extract the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde051e8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e726a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'International Cricket Council'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## impoting necessary libraries\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "## initiating a chain, with the model and the parser\n",
    "chain = model | parser\n",
    "\n",
    "## testing the chain\n",
    "chain.invoke(\"What is the full form of ICC in cricket?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9990fc4a",
   "metadata": {},
   "source": [
    "### 2. Introducing `Prompts` templates\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42b09fc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Human: \\n        Answer the question below based on the context provided. If you don't know the answer, reply with 'I don't know'.\\n         \\n        Context: Ram has 5 apples, one of them is rotten.He threw it out of the window.\\n\\n        Question: How many apples does Ram have?\\n\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We want to provide the model some context and then will ask some questions related to the context\n",
    "'''\n",
    "\n",
    "## importing necessary libraries\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "## creating a template\n",
    "template = \"\"\"\n",
    "        Answer the question below based on the context provided. If you don't know the answer, reply with 'I don't know'.\n",
    "         \n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "## getting the prompt\n",
    "prompt.format(question=\"How many apples does Ram have?\",\n",
    "              context=\"Ram has 5 apples, one of them is rotten.He threw it out of the window.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708d6aee",
   "metadata": {},
   "source": [
    "Prompt Template is working fine. \n",
    "\n",
    "So, our model will work like, first the `prompt` will be provided both with the `question` and `context`, next a `query` will be run and the `model` will answer to that `query` and the `response` will be passed through the `parser` for `final answer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44283dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Indian Cricket Council'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## new chain, first prompt, then model, and then parser\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\n",
    "    \"context\":\"Full form of ICC is Indian Cricket Council.\",\n",
    "    \"question\":\"What is the full form of ICC?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d36f08e",
   "metadata": {},
   "source": [
    "**Conversion from One Language to another language**\n",
    "\n",
    "chain will look like, the `prompt` will receive both `question` and `answer` and then the `prompt` will be forwarded to the `model`. The `response` we get, after providing `query` to the model, will pass through the `parser` and the result we receive from `parser` will be the input for the new `chain` where, the `prompt` will only accept the language name and the answer from previous reply. And the after passing through new `translation prompt`, the `model` will be responsible for providing the expected output after accepting the `query`, and finally the `parser` will give the final result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33568efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating translation template/prompt\n",
    "\n",
    "translation_template = \"\"\"\n",
    "   Trnaslate the following sentence to the new language.\n",
    "\n",
    "   Sentence: {sentence}\n",
    "\n",
    "   New Language: {language}\n",
    "\"\"\"\n",
    "## prompt\n",
    "translational_prompt = ChatPromptTemplate.from_template(translation_template)\n",
    "\n",
    "## testing the prompt\n",
    "# translational_prompt.format(sentence=\"I love you\", language=\"Bengali\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40455f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' কটক, ওড়িশা'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "## creating translation chain\n",
    "translational_chain = (\n",
    "    {\"sentence\":chain, \"language\":itemgetter(\"language\")} | translational_prompt | model | parser\n",
    ")\n",
    "\n",
    "translational_chain.invoke({\n",
    "    \"context\":\"Netaji Subhas Chandra Bose was a great freedom fighter. He was born in Cuttack, Odisha. He was the leader of the Indian National Army.\",\n",
    "    \"question\":\"What is the birthplace of Netaji Subhas Chandra Bose?\",\n",
    "    \"language\":\"Bengali\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e944115",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3497421f",
   "metadata": {},
   "source": [
    "### 3. Transcribing the Youtube Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8049b8",
   "metadata": {},
   "source": [
    "OpenAI Whisper, python module - `whisper` can be used to transcribe any video. And the video script will be used as the context for the model to search result from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "293968b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc97412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daace048",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setting up whisper\n",
    "\n",
    "import whisper\n",
    "import tempfile\n",
    "from pytube import YouTube\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Use CUDA, if available\n",
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "## various variables\n",
    "YOUTUBE_VIDEO_URL = \"https://youtu.be/NZZ0A0OWlkE?si=7cADG-K-tIOG7eeN\"\n",
    "\n",
    "\n",
    "## create trnascroption file only if the file is not already present\n",
    "# transcription_file = os.path.join(os.getcwd(), \"transcription.txt\")\n",
    "if not os.path.exists(\"transcription.txt\"):\n",
    "    youtube = YouTube(YOUTUBE_VIDEO_URL)\n",
    "    audio = youtube.streams.filter(only_audio=True).first()\n",
    "    # print(audio)\n",
    "\n",
    "    # Load the desired model\n",
    "    # whisper_model = whisper.load_model(\"medium.en\").to(DEVICE)\n",
    "    whisper_model = whisper.load_model(\"base.en\")\n",
    "    with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        file = audio.download(output_path=tmpdir)\n",
    "        # file = audio.download(filename=\"sample.mp3\")\n",
    "        print(file) ## Debugging\n",
    "        transcription = whisper_model.transcribe(file, fp16 = False)[\"text\"]\n",
    "\n",
    "    with open(\"transcription.txt\", \"w\") as file:\n",
    "        file.write(transcription)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ae39856",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reading the text file [transcription.txt]\n",
    "\n",
    "with open(\"transcription.txt\", \"r\") as file:\n",
    "    transcription = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7364df4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fa0424c",
   "metadata": {},
   "source": [
    "### 4. Using the Entire Transcription as Context\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5ba0617",
   "metadata": {},
   "outputs": [],
   "source": [
    "##ctesting out one question based on whole context\n",
    "\n",
    "try:\n",
    "    chain.invoke({\n",
    "        \"question\":\"What the speaker is talking about?\",\n",
    "        \"context\":transcription\n",
    "    })\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a43c4c",
   "metadata": {},
   "source": [
    "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 47047 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c1a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d20a031a",
   "metadata": {},
   "source": [
    "### 5. Splitting the Transcription\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a784d04",
   "metadata": {},
   "source": [
    "As we can't use the whole transcription as context so we will be chunking the whole transcription into smaller chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5b8923",
   "metadata": {},
   "source": [
    "Loading the transcription into the memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53144964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"transcription.txt\")\n",
    "text_document = loader.load()\n",
    "# text_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156906bb",
   "metadata": {},
   "source": [
    "For our application, we will be using TextSplitter to create chunks of 1000 tokens, and 10% of overlapping for context remember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8f6da44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## importing libraries\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "## aplying the text splitter\n",
    "documents = text_splitter.split_documents(text_document)\n",
    "len(documents) ## Total No of Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188af51c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db32c858",
   "metadata": {},
   "source": [
    "### 6. Finding the Relevant Chunks\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87cca79",
   "metadata": {},
   "source": [
    "Given a particular question, we need to find the relevant chunks from the transcription to send to the model. Here is where the idea of embeddings comes into play."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d9e7f1",
   "metadata": {},
   "source": [
    "To provide with the most relevant chunks, we can use the embeddings of the question and the chunks of the transcription to compute the similarity between them. We can then select the chunks with the highest similarity to the question and use them as the context for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ed12294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Length: 768\n",
      "[0.040527157, -0.038909737, -0.050615028, -0.02892944, 0.065502904]\n"
     ]
    }
   ],
   "source": [
    "## generating embeddings for the chunks\n",
    "\n",
    "\n",
    "## importig necessary libraries\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",\n",
    "                                          google_api_key=GENAI_API_KEY)\n",
    "\n",
    "vector = embedding.embed_query(\"Swiggy is a food and grocery delivery company, headquartered in Bangalore, India\")\n",
    "print(f\"Embedding Length: {len(vector)}\")\n",
    "print(vector[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45ae342a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## simple comparison of embeddings\n",
    "vector1 = embedding.embed_query(\"India is a country in Asia.\")\n",
    "vector2 = embedding.embed_query(\"Zomato is a food delivery company. It is headquartered in Gurgaon, India.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9649e87a",
   "metadata": {},
   "source": [
    "We can now compute the similarity between the query and each of the two sentences. The closer the embeddings are, the more similar the sentences will be.\n",
    "\n",
    "We can use Cosine Similarity to calculate the similarity between the query and each of the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4165bff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7872207363533565, 0.8508268798257821)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing cosine similarity \n",
    "\n",
    "# from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "query_sentence1_similarity = cosine_similarity([vector], [vector1])[0][0]\n",
    "query_sentence2_similarity = cosine_similarity([vector], [vector2])[0][0]\n",
    "\n",
    "query_sentence1_similarity, query_sentence2_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574a9237",
   "metadata": {},
   "source": [
    "### 6. Setting Up a Vector Store \n",
    "---------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e4ff5",
   "metadata": {},
   "source": [
    "Embedding model will be applied on all chunks and embedding vectors will be stored in a vector Store[VectorDB]\n",
    "\n",
    "A vector store is a database of embeddings that specializes in fast similarity searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd7732f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing necessary libraries\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# vectorstore = FAISS.from_documents(all_splits, embeddings)\n",
    "\n",
    "vectorstore1 = FAISS.from_texts(\n",
    "    [\n",
    "        \"Aman and Somen are brothers.\",\n",
    "        \"Aman has a sister whose name is Riya.\",\n",
    "        \"Riya is a doctor.\",\n",
    "        \"Sayan loves to play football.\",\n",
    "        \"Audi is a luxury car brand.\",\n",
    "        \"Somen is a software engineer.\",\n",
    "        \"Aman and Riya are siblings.\"\n",
    "    ],\n",
    "    embedding=embedding\n",
    ")\n",
    "\n",
    "# vectorstore1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "512d8f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Somen is a software engineer.'), 0.36253956),\n",
       " (Document(page_content='Aman and Somen are brothers.'), 0.42956394),\n",
       " (Document(page_content='Sayan loves to play football.'), 0.55261135),\n",
       " (Document(page_content='Aman has a sister whose name is Riya.'), 0.6138234)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## searching similar documents\n",
    "\n",
    "retriever1 = vectorstore1.as_retriever()\n",
    "vectorstore1.similarity_search_with_score(query = \"What is the profession of Somen?\", top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2604418",
   "metadata": {},
   "source": [
    "### 7. Connecting `VectorStore` to the Chain\n",
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7991aebc",
   "metadata": {},
   "source": [
    "We can use the vector store to find the most relevant chunks from the transcription to send to the model. Here is how we can connect the vector store to the chain:\n",
    "\n",
    "\n",
    "We need to configure a Retriever. The retriever will run a similarity search in the vector store and return the most similar documents back to the next step in the chain.\n",
    "\n",
    "We can get a retriever directly from the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f9fe143a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Aman and Somen are brothers.'),\n",
       " Document(page_content='Aman and Riya are siblings.'),\n",
       " Document(page_content='Aman has a sister whose name is Riya.'),\n",
       " Document(page_content='Sayan loves to play football.')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## creating a retriver\n",
    "\n",
    "retriever1 = vectorstore1.as_retriever()\n",
    "retriever1.invoke(\"What is the relation between Sayan and Aman?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cae9b92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "44b96a8c",
   "metadata": {},
   "source": [
    "Our prompt expects two parameters, \"context\" and \"question.\" We can use the retriever to find the chunks we'll use as the context to answer the question.\n",
    "\n",
    "We can create a map with the two inputs by using the RunnableParallel and RunnablePassthrough classes. This will allow us to pass the context and question to the prompt as a map with the keys \"context\" and \"question.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "af8cfb33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='Sayan loves to play football.'),\n",
       "  Document(page_content='Aman and Somen are brothers.'),\n",
       "  Document(page_content='Aman has a sister whose name is Riya.'),\n",
       "  Document(page_content='Aman and Riya are siblings.')],\n",
       " 'question': 'What is the favourite sport of Sayan?'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## importing necessary libraries\n",
    "\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "\n",
    "setup = RunnableParallel(context = retriever1, question=RunnablePassthrough())\n",
    "setup.invoke(\"What is the favourite sport of Sayan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5cc6a178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doctor'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = setup | prompt | model | parser\n",
    "## testing the chain\n",
    "chain.invoke(\"What is the profession of Riya?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6cd0a91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Luxury car'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## another testing\n",
    "chain.invoke(\"What type of car is Audi?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725831aa",
   "metadata": {},
   "source": [
    "### 8. Loading `transcription` into the vector store\n",
    "-------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f3cc36f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initiating the vector store\n",
    "vectorstore_transcription = FAISS.from_documents(documents, embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d6a74",
   "metadata": {},
   "source": [
    "Creating chain for the new vector store,\n",
    "\n",
    "RunnableParallel and RunnablePassthrough will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a9ea1295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'During the Vietnam War'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\":vectorstore_transcription.as_retriever(), \"question\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model \n",
    "    | parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"When did her mom imegrate to the US?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ddbb35c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The speaker's first job was delivering newspapers, and their second job was working at Morrison's, a grocery store.\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Tell me about the speaker's first and second job.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f1135c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "925123db",
   "metadata": {},
   "source": [
    "### 9. Setting Up the `PineCone`\n",
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a1f15b",
   "metadata": {},
   "source": [
    "In practice, we need a vector store that can handle large amounts of data and perform similarity searches at scale. So we will be using `PineCone`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "58715e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing necessary libraries\n",
    "\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "## setting up the pinecone connection\n",
    "index_name = \"ytvideo-transcribing\"\n",
    "\n",
    "pinecone = PineconeVectorStore.from_documents(\n",
    "    documents, \n",
    "    embedding,\n",
    "    index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496dbeeb",
   "metadata": {},
   "source": [
    "Let's now run a similarity search on pinecone to make sure everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7cccf552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"up, my grandpa was a preacher. They had no money, like very, very poor. My dad only could go to college because he joined the Air Force and was able to go on a GI Bill. My mom immigrated to the US during the Vietnam War and came to America with literally nothing. So between the two of their stories, I thought if they come from nothing, they come from war, they come from the Great Depression, and they could make this life for themselves. I can do anything possible because they've given me this great life that I could have that anything is possible here. So if I don't live my life to the fullest, it's almost a disservice to the sacrifices that they made for me to be here. My mom is also Vietnamese. So I grew up with a very much a tiger mom who was like, you have to do these things, go to school, you need to graduate, you need to get a job. But she also taught to me independence. And what she told me was never rely on anyone else for anything. You need to be able to fully support\", metadata={'source': 'transcription.txt'}),\n",
       " Document(page_content=\"know that's what it was. And I always had a love for beauty and my dad used to take me to beauty counters. So, um, growing up with my dad, he didn't know how to do my hair and my makeup. So he would take me to department stores and he would take me to an Estee Lauder counter and say, OK, here's my daughter. Like, can you show her how to use hair and makeup? Can you show her what products to buy? Because he had no idea. What a great dad. I know, he was the best. And so that really was my love of beauty from a really young age. And so I started learning about beauty from the experts, these people at counters who were telling me about products. And that was in middle school and high school. And then I took that love into creating tutorials for myself and then starting our own brand. That's such a great story. And I'm so glad we're doing this background journey because I feel like I think about this all the time. I went to public speaking school from 11 to 17, public speaking drama\", metadata={'source': 'transcription.txt'})]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.similarity_search(\"What is speakers family background?\")[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eaba2b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initiating the chain\n",
    "\n",
    "chain = (\n",
    "    {\"context\":pinecone.as_retriever(), \"question\":RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model \n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bc70d9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The speaker's grandfather was a preacher and they were very poor. The speaker's father was able to go to college because he joined the Air Force and was able to go on a GI Bill. The speaker's mother immigrated to the US during the Vietnam War and came to America with literally nothing.\""
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## final example\n",
    "\n",
    "chain.invoke(\"What is speakers family background?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
